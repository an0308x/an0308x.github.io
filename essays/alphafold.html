<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AlphaFold - Akshay</title>
    <style>
        body {
            font-family: Charter, Georgia, Cambria, "Times New Roman", Times, serif;
            line-height: 1.6;
            max-width: 650px;
            margin: 40px auto;
            padding: 0 20px;
            color: #222;
            background: #fff;
        }
        nav {
            margin-bottom: 3em;
        }
        nav a {
            color: #444;
            text-decoration: none;
        }
        nav a:hover {
            color: #000;
        }
        h1 {
            font-size: 2em;
            margin-bottom: 0.5em;
            font-weight: normal;
        }
        h2 {
            font-size: 1.5em;
            margin-top: 2em;
            margin-bottom: 0.5em;
            font-weight: normal;
        }
        h3 {
            font-size: 1.2em;
            margin-top: 1.5em;
            margin-bottom: 0.5em;
            font-weight: normal;
        }
        p {
            margin-bottom: 1em;
        }
        .date {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 2em;
        }
        .abstract {
            font-style: italic;
            color: #444;
            margin-bottom: 2em;
            padding-left: 1em;
            border-left: 3px solid #ddd;
        }
        code {
            background: #f5f5f5;
            padding: 0.2em 0.4em;
            border-radius: 3px;
            font-family: "SF Mono", Monaco, "Cascadia Code", "Roboto Mono", Consolas, "Courier New", monospace;
            font-size: 0.9em;
        }
        pre {
            background: #f5f5f5;
            padding: 1em;
            border-radius: 5px;
            overflow-x: auto;
            margin: 1.5em 0;
        }
        pre code {
            background: none;
            padding: 0;
        }
        blockquote {
            margin: 1.5em 0;
            padding-left: 1em;
            border-left: 3px solid #ddd;
            color: #444;
        }
        ul, ol {
            margin-bottom: 1em;
            padding-left: 2em;
        }
        li {
            margin-bottom: 0.5em;
        }
        @media (max-width: 600px) {
            body {
                margin: 20px auto;
            }
        }
    </style>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <nav>
        <a href="/essays.html">← Essays</a>
    </nav>

    <header>
        <h1>AlphaFold</h1>
        <div class="date">December 2024</div>
    </header>

    <main>
        <div class="abstract">
            A comprehensive exploration of AlphaFold's revolutionary approach to protein structure prediction, combining invariant point attention mechanisms with deep understanding of protein fitness landscapes.
        </div>

        <h2>Introduction</h2>
        <p>
            AlphaFold represents one of the most significant breakthroughs in computational biology, solving the decades-old protein folding problem with unprecedented accuracy. This essay explores the key components that make AlphaFold successful: its innovative use of invariant point attention mechanisms and its deep understanding of protein fitness landscapes.
        </p>

        <h2>The Central Dogma of Biology and Protein Structure</h2>
        <p>
            Essentially, the central dogma of biology explains the flow of genetic information in a biological system and its forward path consists of three steps:
        </p>
        <ol>
            <li><strong>DNA Replication:</strong> The DNA is replicated (inside the nucleus). A DNA cell makes an identical copy of its genome before it divides itself into two separate chains.</li>
            <li><strong>Transcription:</strong> The DNA chain is used as a template to produce a complementary RNA copy in a process called transcription. This will again happen inside the nucleus.</li>
            <li><strong>Translation:</strong> The RNA chain is decoded and translated by ribosomes to produce a polypeptide sequence, otherwise known as a protein. This process is called translation and is happening outside of the nucleus.</li>
        </ol>
        <div class="figure">
            <em>[central-dogma-of-biology]<br/>
            An illustration showing the flow of information between DNA, RNA and protein. Image credit: Genome Research Limited</em>
        </div>
        <p>
            The 4 fundamental DNA elements, called bases, are {A, T, C, G} and they will be our sequence elements later on! RNA consists of 4 bases as well but T is replaced with U: {A, U, C, G}. Every DNA or RNA sequence is formulated as a combination of these 4 bases.
        </p>
        <p>
            <em>The following animation can illustrate the process in great detail:</em>
        </p>
        <blockquote>Animation placeholder: Central Dogma Animation</blockquote>
        <h3>Proteins, Amino Acids, Nucleotides and Codons</h3>
        <p>
            By now you get the idea that cells generate proteins, which are sequences of amino acids. A polypeptide is a polymer of amino acids joined together by peptide bonds. In other words, amino acids are the structural elements of all proteins. There are many known amino acids but only 20 are encoded by the universal genetic code. Genetic code refers to all the valid combinations of nucleotide bases. Specifically, 3 adjacent nucleotides constitute a unit known as the codon, which codes for an amino acid.
        </p>
        <p>
            For example, the sequence AUG (in the mRNA) is a codon that specifies the amino acid methionine, which almost always specifies the beginning of a protein. There are 64 possible codons. Out of them, there are 3 codons that do not code for amino acids but indicate the end of a protein. The remaining 61 codons specify the 20 amino acids that make up proteins. Below is the table that demonstrates the valid combinations.
        </p>
        <div class="figure">
            <em>[genetic-code-image]<br/>
            Source: Biology Pictures Blog</em>
        </div>
        <p><em>The following video clarifies many aspects on proteins:</em></p>
        <blockquote>Video placeholder: Protein Structure and Function</blockquote>
        <p>We find it very important to highlight the different visualization of the proteins, as described in the video:</p>
        <div class="figure">
            <em>[protein-3d-visualization]<br/>
            Source</em>
        </div>
        <p>If you are curious to know all the amino acids names and symbols, consult this table.</p>
        <h3>The 4 Levels of Protein Structures</h3>
        <p>To describe a protein’s structure, we use 4 distinct levels. The following figure sums up the protein structural levels from a biological standpoint:</p>
        <div class="figure">
            <em>[protein-structures]<br/>
            Source: Figure after © 2010 PJ Russell, iGenetics 3rd ed.; all text material © 2014 by Steven M. Carr</em>
        </div>
        <ol>
            <li><strong>Primary structure:</strong> Simply the sequence of amino acids in a polypeptide chain.<br/>
                <div class="figure">
                    <em>[primary-structure]<br/>
                    The hormone cow insulin has two polypeptide chains, A and B, shown in diagram below. Each chain has its own set of amino acids, assembled in a particular order. Source and Image credit: OpenStax Biology.</em>
                </div>
            </li>
            <li><strong>Secondary structure:</strong> The 3D arrangement of stable local repeating structures. The two main types are the α-helix and the β-structures (beta sheets).<br/>
                <div class="figure">
                    <em>[alpha-helix-vs-beta-sheets]<br/>
                    Alpha helices vs beta sheets. Source: bioninja.com</em>
                </div>
            </li>
            <li><strong>Tertiary structure:</strong> The overall 3-D shape of the protein molecule, describing the spatial relationship of the secondary structures to one another.</li>
            <li><strong>Quaternary structure:</strong> Describes protein-protein interactions in closely packed arrangements. Not all proteins have a quaternary structure.<br/>
                <div class="figure">
                    <em>[tertiary-structure-proteins]<br/>
                    An example of a protein with a quaternary structure is haemoglobin (O2 carrying molecule in red blood cells)Source: bioninja.com</em>
                </div>
            </li>
        </ol>
        <h3>A Closer Look at Protein Folding: Domains, Motifs, Residues and Turns</h3>
        <ul>
            <li><strong>Domains:</strong> A (structural) domain is a self-stabilizing part of the protein's polypeptide chain. It often folds independently of the rest of the protein chain and is responsible for a specific self-contained job in the protein.<br/>
                <div class="figure">
                    <em>[protein-domains]<br/>
                    The structures of two different proteins shown below share a common PH (Pleckstrin Homology) domain (maroon). Source: LibreTexts:Protein Domains, Motifs, and Folds in Protein Structure</em>
                </div>
            </li>
            <li><strong>Structural motifs:</strong> Small structure-dependent 3D regions formed from connecting different secondary structural elements (e.g. α-helices and β-sheets). Motifs do not retain their function when separated from the larger protein.<br/>
                <div class="figure">
                    <em>[example-of-motifs]<br/>
                    Commonly observed motifs Source:Principles of Biochemistry</em>
                </div>
                <div class="figure">
                    <em>[primary-structure-motifs]<br/>
                    Source: Berg JM, Tymoczko JL, Stryer L. Biochemistry. 5th edition.</em>
                </div>
            </li>
            <li><strong>Residues and side chains:</strong> A residue is a single molecular unit within a polypeptide (an amino acid). The defining feature of an amino acid is its side chain, which influences its properties and interactions.<br/>
                <div class="figure">
                    <em>[residues-and-side-chains]<br/>
                    The defining feature of an amino acid is its side chain (at top, blue circle; below, all colored circles). Source:Nature Education, 2010.</em>
                </div>
            </li>
            <li><strong>Turns and loops:</strong> Types of secondary protein structures that connect α-helices and β-strands, usually causing a change in direction of the polypeptide chain.<br/>
                <div class="figure">
                    <em>[example-of-turns-aminoacids]<br/>
                    Example of motifs. Left: A β-hairpin motif. Right: Helix-Turn-Helix (HTH) motif.</em>
                </div>
            </li>
        </ul>
        <h3>What is a Distogram?</h3>
        <p>
            The distogram is the key intermediate step to protein folding. For a sequence of length L, a 3D distogram (distance + histogram) is an LxL matrix, which shows the histogram of the pairwise distances. The distances are “binned” so it is regarded as a binned distance distribution. Distograms are also called contact maps and they are always symmetric matrices.
        </p>
        <div class="figure">
            <em>[distogram]<br/>
            Source:Deep Learning-Based Advances in Protein Structure Prediction, Pakhrin et al 2021</em>
        </div>
        <blockquote>
            “The problem of protein distogram or real-valued distance prediction (bottom row) is similar to the depth prediction problem in computer vision (top row). In all these problems, the input to the Deep Learning model is a volume (3D tensor). In computer vision, 2D images expand as a volume because of the RGB channels. Similarly, in the case of distance prediction, predicted 1D and 2D features are transformed and packed into 3D volume with many channels of inter-residue information” ~ Subash C. Pakhrin et al. 2021
        </blockquote>
        <p>
            But why distograms? Well, the distances in a distogram are relative, meaning that the inter-residue distances are invariant to 3D rotations and translations. This makes the task much simpler.
        </p>
        <h3>Genotype vs Phenotype</h3>
        <p>
            In a nutshell, an organism’s genotype is the set of genes that it carries while the phenotype is all of its observable characteristics; which are influenced both by its genotype and by the environment.
        </p>
        <blockquote>
            In a supervised machine learning setup, the phenotype is often the target variable.
        </blockquote>
        <h3>Biological Tasks That Can Be Approached with Machine Learning</h3>
        <ul>
            <li><strong>MSA (Multiple Sequence Alignment):</strong> MSA refers to the process or the result of sequence alignment of three or more biological sequences, generally a protein, DNA, or RNA. AlphaFold heavily relies on MSA as an additional input. Instead of the particular primary structure of the protein (target), it finds multiple similar proteins and aligns them together. This process can be regarded as additional information for the model that will work like hand-crafted features.<br/>
                <div class="figure">
                    <em>[aligned-sequences]<br/>
                    Source:Wikipedia</em>
                </div>
                <p>
                    MSA provides a hint towards the fact that the 3D structure is more conserved than the primary sequence structure. The sequence can be slightly altered but the overall structure that is related to protein function is preserved!
                </p>
            </li>
            <li><strong>Protein 3D Structure Prediction:</strong> The inference of the 3D structure of a protein from its amino acid sequence (input). In biological terms, the 3D structure is the secondary and tertiary structure (output).<br/>
                <div class="figure">
                    <em>[protein-contact-prediction]<br/>
                    Two globular proteins with some contacts in them shown in black dotted lines along with the contact distance in Armstrong. The alpha helical protein 1bkr (left) has many long-range contacts and the beta sheet protein 1c9o (right) has more short- and medium-range contacts. Source: Protein Residue Contacts and Prediction Methods</em>
                </div>
            </li>
            <li><strong>Genotype to Phenotype Prediction:</strong> Predicting phenotypes from genotypes, e.g., toehold switch performance from RNA sequences.<br/>
                <div class="figure">
                    <em>[riboregulators]<br/>
                    Source: Sequence-to-function deep learning frameworks for engineered riboregulators, Valeri et al (2020)</em>
                </div>
            </li>
        </ul>
        <h3>Representing DNA and Amino Acid Sequences</h3>
        <p>
            In the DNA world, we have a character-level encoding. Each DNA sequence is a word and each token is a character from the set {A, T, G, C} for DNA and {A, U, G, C} for RNA. In amino acids, we have 20 elements in our dictionary, as there are only 20 amino acids that can be produced from the human genome.
        </p>
        <p>
            A different approach is to use n-gram-like representations (k-mers). For example, for the DNA sequence TAGACTGTC, we get five possible overlapping 5-mers: {TAGAC, AGACT, GACTG, ACTGT, CTGTC}. This creates a different form of embedding, where our vectors contain the number each k-mer occurs for a given sequence.
        </p>
        <h3>Association of Biology with ML Model Design</h3>
        <ul>
            <li><strong>Attention Mechanism for Processing MSA:</strong> The MSA Transformer modifies the attention mechanism to process a set of MSA sequences using tied row attention and axial attention.<br/>
                <div class="figure">
                    <em>[attention-msa]<br/>
                    Source:MSA Transformer, Roshan Rao et al.</em>
                </div>
                <div class="figure">
                    <em>[axial-attention]<br/>
                    Source: Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation</em>
                </div>
            </li>
            <li><strong>AlphaFold2 Core Self-Attention Module: Invariant Point Attention:</strong> The core engineering on AlphaFold2 was the design of a transformer architecture that respects the particularities of the 3D domain and proteins. The DeepMind team proposed the Invariant Point Attention (IPA) module, which is invariant under global Euclidean transformations (roto-translations).<br/>
                <div class="figure">
                    <em>[Invariant-point-attention]<br/>
                    Invariant point attention. Top blue arrays: Invariant Point Attention Module. Middle, red arrays: modulation by the pair representation. Bottom, green arrays: standard attention on abstract features. Dimensions: r: residues, c: channels, h: heads, p: points. Source: AlphaFold2 supplementary material</em>
                </div>
            </li>
        </ul>
        <p>
            Be sure to check our detailed article on vanilla self-attention to grasp a high-level overview of the core AlphaFold2 module. For more information on equivariance we suggest Fabian Fuchs & Justas Dauparas blogpost.
        </p>

        <h2>Invariant Point Attention</h2>
        
        <p>
            Attention mechanisms have revolutionized deep learning across various domains, from natural language processing to computer vision. However, when dealing with 3D geometric data, particularly molecular structures, traditional attention mechanisms face significant challenges. The need to maintain geometric invariance—ensuring that the model's predictions remain consistent under rotations and translations of the input—has led to the development of specialized attention mechanisms known as invariant point attention (IPA).
        </p>

        <p>
            Invariant point attention was first introduced in the context of protein structure prediction, specifically in AlphaFold2, where it plays a crucial role in modeling the complex 3D relationships between amino acid residues. The key insight is that while the absolute positions of atoms in 3D space may vary, the relative geometric relationships between them contain the essential information for understanding molecular structure and function.
        </p>

        <h3>Geometric Invariance</h3>

        <p>
            At the heart of invariant point attention lies the concept of geometric invariance. Consider a protein molecule in 3D space. If we rotate or translate this molecule, its biological function remains unchanged. Therefore, any computational model that aims to understand protein structure must produce the same output regardless of how the molecule is oriented in space.
        </p>

        <p>
            Traditional attention mechanisms compute attention weights based on absolute positions or learned embeddings, which are not invariant to geometric transformations. Invariant point attention addresses this by computing attention based on relative geometric features that are inherently invariant to rotations and translations.
        </p>

        <h3>Mathematical Formulation</h3>

        <p>
            The invariant point attention mechanism can be formulated as follows. Given a set of points in 3D space with associated features, the attention weights are computed using relative geometric information rather than absolute positions.
        </p>

        <p>
            For two points <code>i</code> and <code>j</code> with positions <code>p_i</code> and <code>p_j</code>, the relative distance vector is:
        </p>

        <pre><code>d_ij = p_j - p_i</code></pre>

        <p>
            The attention weight between these points is computed as:
        </p>

        <pre><code>α_ij = softmax(Q_i^T K_j + φ(d_ij))</code></pre>

        <p>
            where <code>Q_i</code> and <code>K_j</code> are query and key vectors, and <code>φ(d_ij)</code> is a learned function that maps the relative distance vector to a scalar value. This formulation ensures that the attention weights depend only on relative geometric relationships, making them invariant to global rotations and translations.
        </p>

        <h3>Applications in Protein Folding</h3>

        <p>
            The most prominent application of invariant point attention is in protein structure prediction, particularly in the AlphaFold2 system. Proteins are complex 3D structures composed of amino acid chains that fold into specific conformations to perform their biological functions. Predicting these 3D structures from amino acid sequences is one of the most challenging problems in computational biology.
        </p>

        <p>
            In AlphaFold2, invariant point attention is used in the structure module to model the relationships between different parts of the protein. The attention mechanism helps the model understand how distant amino acids interact through the 3D structure, even when they are far apart in the linear amino acid sequence.
        </p>

        <h3>Implementation Considerations</h3>

        <p>
            Implementing invariant point attention requires careful consideration of several factors:
        </p>

        <ul>
            <li><strong>Coordinate frames:</strong> The choice of local coordinate frames for computing relative geometric features can significantly impact performance.</li>
            <li><strong>Distance functions:</strong> The function φ that maps relative distances to attention contributions must be carefully designed to capture relevant geometric relationships.</li>
            <li><strong>Computational efficiency:</strong> Computing pairwise geometric relationships can be computationally expensive, requiring optimization strategies.</li>
        </ul>

        <h2>Protein Fitness Landscape</h2>

        <p>
            Understanding protein fitness landscapes is crucial for AlphaFold's ability to predict not just protein structures, but also their functional properties and evolutionary constraints. A protein's fitness landscape represents the relationship between its sequence and its functional performance, providing insights into which mutations are likely to be tolerated or beneficial.
        </p>

        <h3>Understanding Fitness Landscapes</h3>
        <p>
            Protein fitness landscapes are high-dimensional surfaces that map protein sequences to their functional fitness. These landscapes are characterized by peaks (high-fitness sequences) and valleys (low-fitness sequences), with the topology of the landscape determining how proteins can evolve and adapt.
        </p>

        <p>
            The concept of sequence space is fundamental to understanding fitness landscapes. For a protein of length N, the sequence space consists of all possible combinations of amino acids at each position. This space is astronomically large—for a typical protein of 300 amino acids, there are 20^300 possible sequences.
        </p>

        <h3>The Concept of Sequence Space</h3>
        <p>
            Proteins navigate this vast sequence space through evolution, with natural selection acting as a search algorithm that explores the fitness landscape. The challenge is that most random mutations are deleterious, and only a small fraction of sequence changes lead to improved function.
        </p>

        <p>
            AlphaFold leverages this understanding by incorporating evolutionary information from multiple sequence alignments. By analyzing how proteins have evolved across different species, the model can infer which parts of the sequence are functionally important and which mutations are likely to be tolerated.
        </p>

        <h3>Evolutionary Dynamics</h3>
        <p>
            The evolutionary dynamics of proteins across fitness landscapes are complex and influenced by multiple factors. Proteins can evolve through various mechanisms:
        </p>

        <ul>
            <li><strong>Point mutations:</strong> Single amino acid changes that can have varying effects on function</li>
            <li><strong>Insertions and deletions:</strong> Changes in protein length that can affect structure and function</li>
            <li><strong>Domain shuffling:</strong> Recombination of functional domains to create new proteins</li>
            <li><strong>Gene duplication:</strong> Creation of paralogous proteins that can evolve new functions</li>
        </ul>

        <h3>Local Optima and Global Optima</h3>
        <p>
            Fitness landscapes often contain multiple local optima—sequences that are better than their immediate neighbors but not necessarily the best possible sequence. This creates challenges for evolution, as proteins can become trapped in suboptimal configurations.
        </p>

        <p>
            AlphaFold's understanding of fitness landscapes helps it predict not just the most stable structure, but also alternative conformations that might be accessible through evolution. This is crucial for understanding protein dynamics and allostery.
        </p>

        <h3>Computational Approaches</h3>
        <p>
            Modern computational approaches to studying fitness landscapes combine multiple techniques:
        </p>

        <ul>
            <li><strong>Deep mutational scanning:</strong> Systematic mutation of protein positions to map fitness effects</li>
            <li><strong>Phylogenetic analysis:</strong> Using evolutionary relationships to infer functional constraints</li>
            <li><strong>Molecular dynamics:</strong> Simulating protein dynamics to understand conformational flexibility</li>
            <li><strong>Machine learning:</strong> Using neural networks to predict fitness from sequence and structure</li>
        </ul>

        <h3>Deep Learning in Fitness Prediction</h3>
        <p>
            AlphaFold incorporates deep learning techniques to predict not just protein structure, but also functional properties. By training on large datasets of protein sequences and their known structures, the model learns to recognize patterns that correlate with protein function and stability.
        </p>

        <p>
            The model can predict various properties including:
        </p>

        <ul>
            <li><strong>Stability:</strong> How well a protein folds and maintains its structure</li>
            <li><strong>Function:</strong> The biological activity of the protein</li>
            <li><strong>Interactions:</strong> How the protein interacts with other molecules</li>
            <li><strong>Evolutionary constraints:</strong> Which mutations are likely to be tolerated</li>
        </ul>

        <h3>Applications in Protein Engineering</h3>
        <p>
            Understanding protein fitness landscapes has direct applications in protein engineering and design. By predicting how mutations affect protein structure and function, researchers can:
        </p>

        <ul>
            <li><strong>Design novel proteins:</strong> Create proteins with new or improved functions</li>
            <li><strong>Optimize existing proteins:</strong> Improve the properties of natural proteins</li>
            <li><strong>Predict drug resistance:</strong> Understand how pathogens might evolve resistance to therapeutics</li>
            <li><strong>Guide directed evolution:</strong> Design more effective mutagenesis strategies</li>
        </ul>

        <h2>Integration in AlphaFold</h2>

        <p>
            The power of AlphaFold lies in its integration of multiple approaches. The invariant point attention mechanism provides the geometric understanding necessary to model 3D protein structures, while the fitness landscape analysis provides the evolutionary context that guides structure prediction.
        </p>

        <p>
            This integration allows AlphaFold to:
        </p>

        <ul>
            <li>Predict protein structures with unprecedented accuracy</li>
            <li>Understand the functional implications of structural predictions</li>
            <li>Identify regions of proteins that are functionally important</li>
            <li>Predict how mutations might affect protein structure and function</li>
            <li>Guide protein engineering and design efforts</li>
        </ul>

        <h2>Future Directions</h2>

        <p>
            The success of AlphaFold opens up exciting possibilities for future research:
        </p>

        <ul>
            <li><strong>Protein dynamics:</strong> Extending predictions to include protein motion and conformational changes</li>
            <li><strong>Protein complexes:</strong> Predicting the structures of protein-protein interactions</li>
            <li><strong>Membrane proteins:</strong> Improving predictions for challenging membrane protein structures</li>
            <li><strong>Drug design:</strong> Using structural predictions to guide small molecule drug discovery</li>
            <li><strong>Personalized medicine:</strong> Predicting the effects of genetic variants on protein function</li>
        </ul>

        <h2>Conclusion</h2>

        <p>
            AlphaFold represents a paradigm shift in computational biology, combining cutting-edge machine learning techniques with deep biological insights. The integration of invariant point attention and fitness landscape analysis provides a comprehensive framework for understanding protein structure and function.
        </p>

        <p>
            As we continue to develop more sophisticated models and gather more data, we can expect to see even more remarkable advances in our ability to predict and understand protein structures. This will have profound implications for drug discovery, protein engineering, and our fundamental understanding of biology.
        </p>

        <h2>References</h2>
        <p>
            [References to AlphaFold papers, invariant point attention literature, and protein fitness landscape studies]
        </p>
    </main>
</body>
</html> 